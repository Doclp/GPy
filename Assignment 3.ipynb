{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing assignment #3, you should be able to:\n",
    "\n",
    "- understand the basics of GPy\n",
    "\n",
    "\n",
    "- use GPy for Gaussian process regression\n",
    "\n",
    "\n",
    "- use GPy for Gaussian process classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as snb\n",
    "\n",
    "import GPy\n",
    "\n",
    "snb.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gaussian process regression using GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to introduce you to regression modelling based on Gaussian processes using the GPy toolbox:\n",
    "\n",
    "\n",
    "https://github.com/SheffieldML/GPy\n",
    "\n",
    "\n",
    "GPy is an open source python module for Gaussian process modelling. It contains efficient and robust implementations of most of the models that we have discussed in the course so far and much more. \n",
    "\n",
    "\n",
    "You need to install GPy in order to complete this assignment. There are several ways to install GPy, but the fastest way is usually using pip, e.g. \"<tt>pip install GPy</tt>\".\n",
    "\n",
    "\n",
    "\n",
    "After installing GPy, the first task in assignment #3 is to go through each step in the following two tutorials\n",
    "\n",
    "\n",
    "1) http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/basic_gp.ipynb\n",
    "\n",
    "\n",
    "2) http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/basic_kernels.ipynb\n",
    "\n",
    "\n",
    "and make sure you understand each step. Download the tutorials and play around with them to make sure you understand. Look up the functions in the documentation if necessary.\n",
    "\n",
    "\n",
    "\n",
    "If you prefer Matlab, you can do more or less the same using the GPStuff toolbox:\n",
    "\n",
    "http://research.cs.aalto.fi/pml/software/gpstuff/\n",
    "\n",
    "But it will require significantly more work from you.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Modelling the airline passenger data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of task 2 is to model the airline passenger data set using Gaussian processes in GPy. Specifically, to model the number of airline passengers as a function of time.\n",
    "\n",
    "\n",
    "The file <tt>AirPassengers.csv</tt> contains the number of airline passengers as a function of time from 1949 to almost 1961. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Load the airline data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to load the data from the file <tt>AirPassengers.csv</tt>. You should complete the implementation of the function given below. The function loads the data from the file and returns two vectors ${\\bf x}$ and ${\\bf y}$, where ${\\bf x}$ is a vector containing the time (in years) and ${\\bf y}$ is a vector containing the corresponding number of airline passengers for each time point.\n",
    "\n",
    "\n",
    "Hint: you can use the function <tt>np.genfromtxt</tt> to load the data and you can use <tt>np.genfromtxt?</tt> to look up the documentation of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    ### Load the airline passenger data set from the file AirPassengers.csv\n",
    "    #\n",
    "    # Arguments: \n",
    "    # None\n",
    "    #\n",
    "    # Returns:\n",
    "    # x   -- (Nx1) vector of time points\n",
    "    # y   -- (Nx1) vector of number of passengers\n",
    "    \n",
    "    ###################################################\n",
    "    ## Insert code here\n",
    "    ###################################################\n",
    "\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b:  Split the data into training and test data and visualize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you should divide the data into a training set and a test set. Speficically, you should use the data before 1958 as training data and the rest of the data as test data. Load the data and split it into training and test sets by creating the vectors $\\left( {\\bf x}_{\\text{train}}, {\\bf y}_{\\text{train}}\\right)$ and $\\left({\\bf x}_{\\text{test}}, {\\bf x}_{\\text{test}}\\right)$.\n",
    "\n",
    "\n",
    "Next, plot the training and test data in the same plot. Plot the training points in red and the test points in blue. Add legends and proper labels to the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c: Gaussian process fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you should fit a Gaussian process model to the training data and make predictions for the test data using GPy. Compute and plot the fit (predicted mean function plus/minus 2 standard deviations) for the squared exponential kernel.\n",
    "\n",
    "What is the marginal likelihood for the model?\n",
    "\n",
    "Compute the average test error and MLPPD for the two models.\n",
    "\n",
    "Hint 1): Look up <tt>GPy.kern.RBF</tt>\n",
    "\n",
    "Hint 2): Use <tt>print(model)</tt> to print useful information about the model. Try to print the model information before and after you optimize the hyperparameters. Check that the values of the hyperparameters match your expectations.\n",
    "\n",
    "Hint 3): Look up the function <tt>model.log_predictive_density</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2d: Experiment with combinations of different kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different kernels and combinations of kernels in GPy. You can see the list of available kernels by: \"<tt>dir(GPy.kern)</tt>\".\n",
    "\n",
    "You can combine two basic kernels into a more complicated kernel using the sum and product rule for kernels (using the + and * operators with GPy kernel objects).\n",
    "\n",
    "What's the minimum average test error and MLPPD that you can achieve?\n",
    "\n",
    "Show the three best solutions that you obtain. Comment on the quality of the fit vs. test error.\n",
    "\n",
    "Hint: If you are using a sum of one or more kernels, it is sometimes useful to initialize their kernel parameters  with different value to capture different features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: The bias kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the task is to understand the meaning and interpretation of the **bias** covariance function.\n",
    "\n",
    "\n",
    "We are usually considering a Gaussian process prior where the mean function $m({\\bf x})$ is assumed to be zero, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bf{f}) = \\mathcal{N}\\left(\\bf{f}\\big|\\bf{0}, \\bf{K}\\right),\n",
    "\\end{align}\n",
    "\n",
    "where ${\\bf K}_{ij} = k({\\bf x}_i, {\\bf x}_j)$ for some covariance function $k$.\n",
    "We'll now consider a GP prior with a constant mean function $m({\\bf x}) = c$ for all ${\\bf x} \\in \\mathbb{R}^D$ and some value $c \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p({\\bf f}) = \\mathcal{N}\\left({\\bf f}\\big|c\\cdot {\\bf 1}, {\\bf K}\\right).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Usually, we don't know the value of the mean function $c$ apriori, so we will assign a prior distribution on $m$. We will assume that $c$ follows a Gaussian distribution with variance $\\tau^2$.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p({\\bf f}, c) = \\mathcal{N}\\left({\\bf f}\\big|c\\cdot {\\bf 1}, {\\bf K}\\right)\\mathcal{N}\\left(c\\big|0, \\tau^2\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can now integrate over $c$ to get the following model\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p({\\bf f}) &= \\int p({\\bf f}, c) \\text{d} c\\\\\n",
    "&= \\int \\mathcal{N}\\left({\\bf f}\\big|c\\cdot {\\bf 1}, {\\bf K}\\right)\\mathcal{N}\\left(c\\big|0, \\tau^2\\right)\\text{d} c\\\\\n",
    "&= \\mathcal{N}\\left({\\bf f}\\big|{\\bf 0}, {\\bf K}+ \\tau^2 {\\bf 1}{\\bf 1}^T \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Thus, adding a constant mean function $m({\\bf x}) = c$ with a Gaussian prior on $c\\sim \\mathcal{N}(0, \\tau^2)$ is equivalent to adding the constant $\\tau^2$ to the covariane function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Interpretation of the bias covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **bias** covariance function is given by: $k({\\bf x}_1, {\\bf x}_2) = \\tau^2$. What kind of functions are modelled by the bias covariance function? Why is preferable to integrate over $c$ and handle $\\tau$ as a kernel parameter rather than a parameter for $c$?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: The effect of the bias covariance function in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below generates a simulated data set $\\left({\\bf x}, {\\bf y}\\right)$. Fit a Gaussian process with a Squared exponential kernel to the data set and compute the posterior distribution for each of the points contained in the vector ${\\bf x}_p$ given below. Remember to optimize the hyperparameters.\n",
    "\n",
    "\n",
    "Repeat the fitting process using the sum of a squared exponential kernel and a bias kernel. Compute the posterior distribution for each of the points contained in the vector ${\\bf x}_p$ given below. Remember to optimize the hyperparameters.\n",
    "\n",
    "What are the valus of the optimized hyperparameters for the two fits?\n",
    "\n",
    "Plot the two posterior distributions side by side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "xp = np.linspace(-0.5, 2.5, 101)[:, None]\n",
    "\n",
    "f = lambda x: 0.3*(x-1)**3 + 25\n",
    "fp = f(xp)\n",
    "\n",
    "\n",
    "N = 30\n",
    "sigma2 = 1e-3\n",
    "x = np.random.normal(0.8, 0.4, size=(N, 1))\n",
    "y = f(x) + np.random.normal(0, np.sqrt(sigma2), size=(N, 1))\n",
    "\n",
    "plt.plot(xp, fp, 'g--', label='True function', linewidth=3)\n",
    "plt.plot(x, y, 'k.', markersize=10, label='Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "\n",
    "# Gaussian process with squared exponential kernel\n",
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "# Gaussian process with squared exponential + bias kernel\n",
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n",
    "\n",
    "\n",
    "# plot posterior distribution for each model\n",
    "plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "###################################################\n",
    "## Insert code here\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c: Explain what you see in the figures from task 2b. What are two differences between the two fits and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer goes here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: 4aussian process classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to investigate how you can do Gaussian process classification using GPy.\n",
    "\n",
    "Below you are given a classification data set with two classes. Data points with label $y_n=-1$ are shown in blue and data points with label $y_n=1$ are shown in red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.load('classification_data.npz')\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "# visualize it\n",
    "def plot_data(X, y):\n",
    "    \n",
    "    c = y[:, 0]\n",
    "    plt.plot(X[c == -1, 0], X[c == -1, 1], 'k.', markersize=18)\n",
    "    plt.plot(X[c == -1, 0], X[c == -1, 1], '.', color=snb.color_palette()[2], markersize=12)\n",
    "\n",
    "    plt.plot(X[c == 1, 0], X[c == 1, 1], 'k.', markersize=18)\n",
    "    plt.plot(X[c == 1, 0], X[c == 1, 1], '.', color=snb.color_palette()[0], markersize=12)\n",
    "\n",
    "    plt.xlabel('Input $x_1$')\n",
    "    plt.ylabel('Input $x_2$')\n",
    "    plt.ylim((-3, 3))\n",
    "    plt.xlim((-3, 3))\n",
    "    \n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a: Setup up the classification model in GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the joint probabilistic model for the observations ${\\bf y}$ and ${\\bf f} = f({\\bf x})$. Explain the components of the model with your own words and explain why Gaussian process classification is computationally more difficult than Gaussian process regression.\n",
    "\n",
    "Gaussian process classification is implemented in the GPy class called <tt>GPy.models.GPClassification</tt>. Fit a model to training data using the squared exponential covariance function. Optimize the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b: Create a grid of input points for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to predict the class probabilities for the entire input space. To facillitate that, create a grid of points in the input space spanned by $(x_1, x_2)$ shown in figure above such. Use 100 points along each dimension such that there are $100^2$ points in the grid in total. Plot the grid points. \n",
    "\n",
    "Create a vector ${\\bf X}_p  \\in \\mathbb{R}^{1000 \\times 2}$ containing all the coordinates of the grid points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4c: The posterior distribution $p(f_*|{\\bf y})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the posterior distribution $p(f_*|{\\bf y}) = \\mathcal{N}(f_*|\\mu_*, \\sigma^2_*)$ for each point in ${\\bf X}_p  \\in \\mathbb{R}^{1000 \\times 2}$. Visualize the posterior mean $\\mu_*$ and $\\sigma^2_*$ as a function for $x_1$ and $x_2$ in two separates plots and superimpose the training data on top of each of the plots using the <tt>plot_data</tt> function given above. \n",
    "\n",
    "Explain what you see.\n",
    "\n",
    "Hint: look at the GPy function called <tt>predict_noiseless</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4d: The predictive distribution $p(y_*|{\\bf y})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predictive distribution $p(y_*|{\\bf y}) = \\text{Ber}(y_*|p_*)$. Visualize the posterior probability $p_*$ as a function for $x_1$ and $x_2$ and superimpose the training data on top of the plots using the <tt>plot_data</tt> function given above. \n",
    "\n",
    "What is the analytical expression for $p_*$ given $mu_*$ and $\\sigma^2_*$?\n",
    "\n",
    "\n",
    "\n",
    "Explain what you see.\n",
    "\n",
    "\n",
    "What is the relationship betwen this figure and the figures from Task 3c?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e: Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the probability that the pont $(-2.5, 0.5)$ and $(0.5, 1)$ belong to the class with labels $-1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Insert code here\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
